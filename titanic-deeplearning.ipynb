{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d079c1a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.191394Z",
     "iopub.status.busy": "2022-06-17T12:01:55.190255Z",
     "iopub.status.idle": "2022-06-17T12:01:55.202409Z",
     "shell.execute_reply": "2022-06-17T12:01:55.201384Z"
    },
    "papermill": {
     "duration": 0.025218,
     "end_time": "2022-06-17T12:01:55.204884",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.179666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/titanic/train.csv\n",
      "/kaggle/input/titanic/test.csv\n",
      "/kaggle/input/titanic/gender_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf4137c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.217328Z",
     "iopub.status.busy": "2022-06-17T12:01:55.216724Z",
     "iopub.status.idle": "2022-06-17T12:01:55.474349Z",
     "shell.execute_reply": "2022-06-17T12:01:55.473231Z"
    },
    "papermill": {
     "duration": 0.266366,
     "end_time": "2022-06-17T12:01:55.476743",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.210377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4dcd8a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.488482Z",
     "iopub.status.busy": "2022-06-17T12:01:55.488092Z",
     "iopub.status.idle": "2022-06-17T12:01:55.493010Z",
     "shell.execute_reply": "2022-06-17T12:01:55.492312Z"
    },
    "papermill": {
     "duration": 0.012982,
     "end_time": "2022-06-17T12:01:55.494810",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.481828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12683a0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.507108Z",
     "iopub.status.busy": "2022-06-17T12:01:55.506079Z",
     "iopub.status.idle": "2022-06-17T12:01:55.511032Z",
     "shell.execute_reply": "2022-06-17T12:01:55.510331Z"
    },
    "papermill": {
     "duration": 0.01297,
     "end_time": "2022-06-17T12:01:55.512850",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.499880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2612b1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.525873Z",
     "iopub.status.busy": "2022-06-17T12:01:55.524623Z",
     "iopub.status.idle": "2022-06-17T12:01:55.530694Z",
     "shell.execute_reply": "2022-06-17T12:01:55.529626Z"
    },
    "papermill": {
     "duration": 0.014647,
     "end_time": "2022-06-17T12:01:55.532575",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.517928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a71e62cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.545510Z",
     "iopub.status.busy": "2022-06-17T12:01:55.544746Z",
     "iopub.status.idle": "2022-06-17T12:01:55.550615Z",
     "shell.execute_reply": "2022-06-17T12:01:55.549749Z"
    },
    "papermill": {
     "duration": 0.01487,
     "end_time": "2022-06-17T12:01:55.552666",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.537796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb1e3d0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.564963Z",
     "iopub.status.busy": "2022-06-17T12:01:55.564281Z",
     "iopub.status.idle": "2022-06-17T12:01:55.572918Z",
     "shell.execute_reply": "2022-06-17T12:01:55.572185Z"
    },
    "papermill": {
     "duration": 0.017392,
     "end_time": "2022-06-17T12:01:55.575254",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.557862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e23e963",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.587255Z",
     "iopub.status.busy": "2022-06-17T12:01:55.586540Z",
     "iopub.status.idle": "2022-06-17T12:01:55.594384Z",
     "shell.execute_reply": "2022-06-17T12:01:55.593364Z"
    },
    "papermill": {
     "duration": 0.016371,
     "end_time": "2022-06-17T12:01:55.596617",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.580246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "948ce514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.608831Z",
     "iopub.status.busy": "2022-06-17T12:01:55.608111Z",
     "iopub.status.idle": "2022-06-17T12:01:55.616591Z",
     "shell.execute_reply": "2022-06-17T12:01:55.615821Z"
    },
    "papermill": {
     "duration": 0.016856,
     "end_time": "2022-06-17T12:01:55.618721",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.601865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bf48ff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.630261Z",
     "iopub.status.busy": "2022-06-17T12:01:55.629869Z",
     "iopub.status.idle": "2022-06-17T12:01:55.637341Z",
     "shell.execute_reply": "2022-06-17T12:01:55.636564Z"
    },
    "papermill": {
     "duration": 0.015724,
     "end_time": "2022-06-17T12:01:55.639382",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.623658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc4afd7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.650868Z",
     "iopub.status.busy": "2022-06-17T12:01:55.650193Z",
     "iopub.status.idle": "2022-06-17T12:01:55.656378Z",
     "shell.execute_reply": "2022-06-17T12:01:55.655517Z"
    },
    "papermill": {
     "duration": 0.014058,
     "end_time": "2022-06-17T12:01:55.658302",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.644244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5035f27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.670136Z",
     "iopub.status.busy": "2022-06-17T12:01:55.669419Z",
     "iopub.status.idle": "2022-06-17T12:01:55.678151Z",
     "shell.execute_reply": "2022-06-17T12:01:55.677402Z"
    },
    "papermill": {
     "duration": 0.017177,
     "end_time": "2022-06-17T12:01:55.680301",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.663124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (isinstance(db, float))\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77cd9cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.691994Z",
     "iopub.status.busy": "2022-06-17T12:01:55.691321Z",
     "iopub.status.idle": "2022-06-17T12:01:55.700335Z",
     "shell.execute_reply": "2022-06-17T12:01:55.699435Z"
    },
    "papermill": {
     "duration": 0.017247,
     "end_time": "2022-06-17T12:01:55.702418",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.685171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "565017a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.714004Z",
     "iopub.status.busy": "2022-06-17T12:01:55.713338Z",
     "iopub.status.idle": "2022-06-17T12:01:55.719813Z",
     "shell.execute_reply": "2022-06-17T12:01:55.719114Z"
    },
    "papermill": {
     "duration": 0.014549,
     "end_time": "2022-06-17T12:01:55.721783",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.707234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "018e6ada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.733825Z",
     "iopub.status.busy": "2022-06-17T12:01:55.733125Z",
     "iopub.status.idle": "2022-06-17T12:01:55.740295Z",
     "shell.execute_reply": "2022-06-17T12:01:55.739532Z"
    },
    "papermill": {
     "duration": 0.015758,
     "end_time": "2022-06-17T12:01:55.742453",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.726695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c8f5eb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.754506Z",
     "iopub.status.busy": "2022-06-17T12:01:55.753797Z",
     "iopub.status.idle": "2022-06-17T12:01:55.765381Z",
     "shell.execute_reply": "2022-06-17T12:01:55.764483Z"
    },
    "papermill": {
     "duration": 0.02018,
     "end_time": "2022-06-17T12:01:55.767559",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.747379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        A1, cache1 = linear_activation_forward(X,W1,b1,'relu')\n",
    "        A2, cache2 = linear_activation_forward(A1,W2,b2,'sigmoid')\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(A2,Y)\n",
    "        \n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2,cache2,'sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1,cache1,'relu')\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35449759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.779599Z",
     "iopub.status.busy": "2022-06-17T12:01:55.778901Z",
     "iopub.status.idle": "2022-06-17T12:01:55.786965Z",
     "shell.execute_reply": "2022-06-17T12:01:55.786311Z"
    },
    "papermill": {
     "duration": 0.016538,
     "end_time": "2022-06-17T12:01:55.788988",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.772450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X,parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    " \n",
    "        grads = L_model_backward(AL,Y,caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3190ae34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.801264Z",
     "iopub.status.busy": "2022-06-17T12:01:55.800559Z",
     "iopub.status.idle": "2022-06-17T12:01:55.805912Z",
     "shell.execute_reply": "2022-06-17T12:01:55.805228Z"
    },
    "papermill": {
     "duration": 0.014021,
     "end_time": "2022-06-17T12:01:55.807820",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.793799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_costs(costs, learning_rate=0.0075):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d56f4666",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-17T12:01:55.820158Z",
     "iopub.status.busy": "2022-06-17T12:01:55.819415Z",
     "iopub.status.idle": "2022-06-17T12:01:57.077647Z",
     "shell.execute_reply": "2022-06-17T12:01:57.076095Z"
    },
    "papermill": {
     "duration": 1.266877,
     "end_time": "2022-06-17T12:01:57.080000",
     "exception": false,
     "start_time": "2022-06-17T12:01:55.813123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'train_test'],\n",
      "      dtype='object')\n",
      "train_x's shape: (41, 889)\n",
      "test_x's shape: (41, 418)\n",
      "Cost after iteration 0: 0.6929199213904303\n",
      "Cost after iteration 100: 0.6629747016341476\n",
      "Cost after iteration 200: 0.6608112481323204\n",
      "Cost after iteration 300: 0.659290721028533\n",
      "Cost after iteration 400: 0.657195544222203\n",
      "Cost after iteration 500: 0.6540911571813659\n",
      "Cost after iteration 600: 0.6494154396619257\n",
      "Cost after iteration 700: 0.6425712829424084\n",
      "Cost after iteration 800: 0.6329960816860136\n",
      "Cost after iteration 900: 0.6206273453426671\n",
      "Cost after iteration 1000: 0.6056137873302023\n",
      "Cost after iteration 1100: 0.5880811235334861\n",
      "Cost after iteration 1200: 0.5673604191551138\n",
      "Cost after iteration 1300: 0.5431687172160858\n",
      "Cost after iteration 1400: 0.5184563462439321\n",
      "Cost after iteration 1500: 0.4959924608456236\n",
      "Cost after iteration 1600: 0.47741564187765706\n",
      "Cost after iteration 1700: 0.4629707883683852\n",
      "Cost after iteration 1800: 0.4521223000051152\n",
      "Cost after iteration 1900: 0.4440708761380671\n",
      "Cost after iteration 2000: 0.43809327175681534\n",
      "Cost after iteration 2100: 0.4336187542196894\n",
      "Cost after iteration 2200: 0.4302066388244894\n",
      "Cost after iteration 2300: 0.4734677102106238\n",
      "Cost after iteration 2400: 0.45113137141249793\n",
      "Cost after iteration 2499: 0.4416950407990458\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8065241844769406\n",
      "Accuracy: 0.880382775119617\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "    plt.rcParams['image.interpolation'] = 'nearest'\n",
    "    plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "\n",
    "    training = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
    "    test = pd.read_csv('/kaggle/input/titanic/test.csv')\n",
    "    \n",
    "#     training = pd.read_csv('train.csv')\n",
    "#     test = pd.read_csv('test.csv')\n",
    "    yo = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\n",
    "    yooo = yo.Survived\n",
    "\n",
    "    training['train_test'] = 1\n",
    "    test['train_test'] = 0\n",
    "    test['Survived'] = np.NaN\n",
    "    all_data = pd.concat([training,test])\n",
    "\n",
    "    print(all_data.columns)\n",
    "\n",
    "    all_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n",
    "    all_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\n",
    "    all_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\n",
    "    all_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\n",
    "    all_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
    "\n",
    "    #impute nulls for continuous data \n",
    "    #all_data.Age = all_data.Age.fillna(training.Age.mean())\n",
    "    all_data.Age = all_data.Age.fillna(training.Age.median())\n",
    "    #all_data.Fare = all_data.Fare.fillna(training.Fare.mean())\n",
    "    all_data.Fare = all_data.Fare.fillna(training.Fare.median())\n",
    "\n",
    "    #drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \n",
    "    all_data.dropna(subset=['Embarked'],inplace = True)\n",
    "\n",
    "    #tried log norm of sibsp (not used)\n",
    "    all_data['norm_sibsp'] = np.log(all_data.SibSp+1)\n",
    "    # all_data['norm_sibsp'].hist()\n",
    "\n",
    "    # log norm of fare (used)\n",
    "    all_data['norm_fare'] = np.log(all_data.Fare+1)\n",
    "    # all_data['norm_fare'].hist()\n",
    "\n",
    "    # converted fare to category for pd.get_dummies()\n",
    "    all_data.Pclass = all_data.Pclass.astype(str)\n",
    "\n",
    "    #created dummy variables from categories (also can use OneHotEncoder)\n",
    "    all_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','cabin_adv','cabin_multiple','numeric_ticket','name_title','train_test']])\n",
    "\n",
    "    #Split to train test again\n",
    "    X_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\n",
    "    X_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n",
    "\n",
    "\n",
    "    y_train = all_data[all_data.train_test==1].Survived\n",
    "    y_train.shape\n",
    "\n",
    "    train_x = X_train.to_numpy().T\n",
    "    test_x = X_test.to_numpy().T\n",
    "    train_y = y_train.to_numpy().reshape(889,1).T\n",
    "    test_y = yooo.to_numpy().reshape(418,1).T\n",
    "\n",
    "    print (\"train_x's shape: \" + str(train_x.shape))\n",
    "    print (\"test_x's shape: \" + str(test_x.shape))\n",
    "\n",
    "    n_x = 41     # num_px * num_px * 3\n",
    "    n_h = 7\n",
    "    n_y = 1\n",
    "    layers_dims = (n_x, n_h, n_y)\n",
    "    learning_rate = 0.0075\n",
    "\n",
    "    parameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)\n",
    "    #layers_dims = [12288, 20, 7, 5, 1] #  4-layer model\n",
    "    #parameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)\n",
    "    plot_costs(costs, learning_rate)\n",
    "\n",
    "    predictions_train = predict(train_x, train_y, parameters)\n",
    "    predictions_test = predict(test_x, test_y, parameters)\n",
    "    y_hat = np.squeeze(predictions_test.astype(int).T)\n",
    "    \n",
    "    final_data = {'PassengerId': test.PassengerId, 'Survived': y_hat}\n",
    "    submission = pd.DataFrame(data=final_data)\n",
    "    submission.to_csv('submission_dl.csv', index =False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.081325,
   "end_time": "2022-06-17T12:01:57.712692",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-17T12:01:45.631367",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
